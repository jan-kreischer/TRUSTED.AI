{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Trusted AI Master Project This site serves as documentation and Overview of the Master Project. Introduction Artificial intelligence (AI) systems are getting more and more relevance as support to human decision-making processes. While AI holds the promise of delivering valuable insights into many application scenarios, the broad adoption of AI systems will rely on the ability to trust their decisions. According to IBM, some key aspects provide information to trust (or not) a decision made by an algorithm. In particular, a decision must: be reliable and fair, be accounted for, not cause harm, not be tampered with, be understandable, and be secure. In this context, the previous aspects have been grouped into the following pillars. \u2022 Fairness. One of the essential aspects to enable trusted AI is to avoid bias across the entire lifecycle of AI applications, as well as across different bias and data types. \u2022 Robustness. AI-based systems may be vulnerable to adversarial attacks. Attackers may poison training data by injecting samples to compromise system decisions eventually. \u2022 Explainability. In many application scenarios, decisions made by algorithms must be explainable and understandable by humans. Various stakeholders require explanations for different purposes, and explanations must be tailored to their needs. \u2022 Accountability. The quality of decisions should be evaluated in terms of accuracy, ability to satisfy users\u2019 preferences, as well as other properties related to the impact of the decision.The previous characteristics are demanded from AI systems. Yet, to achieve trust in AI, more efforts and automatic mechanisms able to calculate and communicate trust levels are required.","title":"Home"},{"location":"#trusted-ai-master-project","text":"This site serves as documentation and Overview of the Master Project.","title":"Trusted AI Master Project"},{"location":"#introduction","text":"Artificial intelligence (AI) systems are getting more and more relevance as support to human decision-making processes. While AI holds the promise of delivering valuable insights into many application scenarios, the broad adoption of AI systems will rely on the ability to trust their decisions. According to IBM, some key aspects provide information to trust (or not) a decision made by an algorithm. In particular, a decision must: be reliable and fair, be accounted for, not cause harm, not be tampered with, be understandable, and be secure. In this context, the previous aspects have been grouped into the following pillars. \u2022 Fairness. One of the essential aspects to enable trusted AI is to avoid bias across the entire lifecycle of AI applications, as well as across different bias and data types. \u2022 Robustness. AI-based systems may be vulnerable to adversarial attacks. Attackers may poison training data by injecting samples to compromise system decisions eventually. \u2022 Explainability. In many application scenarios, decisions made by algorithms must be explainable and understandable by humans. Various stakeholders require explanations for different purposes, and explanations must be tailored to their needs. \u2022 Accountability. The quality of decisions should be evaluated in terms of accuracy, ability to satisfy users\u2019 preferences, as well as other properties related to the impact of the decision.The previous characteristics are demanded from AI systems. Yet, to achieve trust in AI, more efforts and automatic mechanisms able to calculate and communicate trust levels are required.","title":"Introduction"},{"location":"AI_algo/","text":"Trusted AI Algorithm Basic Idea Taxonomy Architecture","title":"trusted AI algorithm"},{"location":"AI_algo/#trusted-ai-algorithm","text":"","title":"Trusted AI Algorithm"},{"location":"AI_algo/#basic-idea","text":"","title":"Basic Idea"},{"location":"AI_algo/#taxonomy","text":"","title":"Taxonomy"},{"location":"AI_algo/#architecture","text":"","title":"Architecture"},{"location":"code_documentation/","text":"Implementation of the trusted AI Algorithm General Architecture File Structure Build How it works Evaluation and Results","title":"Implementation"},{"location":"code_documentation/#implementation-of-the-trusted-ai-algorithm","text":"","title":"Implementation of the trusted AI Algorithm"},{"location":"code_documentation/#general-architecture","text":"","title":"General Architecture"},{"location":"code_documentation/#file-structure","text":"","title":"File Structure"},{"location":"code_documentation/#build","text":"","title":"Build"},{"location":"code_documentation/#how-it-works","text":"","title":"How it works"},{"location":"code_documentation/#evaluation-and-results","text":"","title":"Evaluation and Results"},{"location":"sources/","text":"Overview This is a collection of all material that might be interesting or relevant regarding the project. Papers AI fairness 360: Bellamy, R. K., et al. (2018). AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias. arXiv, October 2018, arXiv:1810.01943. pdf version YouTube Videos Add here a list of usefull and interesing YT videos Decison making: Can we trust artificial intelligence to make decisions for us? Websites IBM trusted AI main page: AI Research","title":"sources"},{"location":"sources/#overview","text":"This is a collection of all material that might be interesting or relevant regarding the project.","title":"Overview"},{"location":"sources/#papers","text":"AI fairness 360: Bellamy, R. K., et al. (2018). AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias. arXiv, October 2018, arXiv:1810.01943. pdf version","title":"Papers"},{"location":"sources/#youtube-videos","text":"Add here a list of usefull and interesing YT videos Decison making: Can we trust artificial intelligence to make decisions for us?","title":"YouTube Videos"},{"location":"sources/#websites","text":"IBM trusted AI main page: AI Research","title":"Websites"},{"location":"Pillars/Accountability/","text":"Accountability Summary Links Taxonomy and Metrics","title":"Accountability"},{"location":"Pillars/Accountability/#accountability","text":"","title":"Accountability"},{"location":"Pillars/Accountability/#summary","text":"","title":"Summary"},{"location":"Pillars/Accountability/#links","text":"","title":"Links"},{"location":"Pillars/Accountability/#taxonomy-and-metrics","text":"","title":"Taxonomy and Metrics"},{"location":"Pillars/Explainability/","text":"Explainability Summary Links Metrics","title":"Explainability"},{"location":"Pillars/Explainability/#explainability","text":"","title":"Explainability"},{"location":"Pillars/Explainability/#summary","text":"","title":"Summary"},{"location":"Pillars/Explainability/#links","text":"","title":"Links"},{"location":"Pillars/Explainability/#metrics","text":"","title":"Metrics"},{"location":"Pillars/Fairness/","text":"Fairness Summary Links Metrics","title":"Fairness"},{"location":"Pillars/Fairness/#fairness","text":"","title":"Fairness"},{"location":"Pillars/Fairness/#summary","text":"","title":"Summary"},{"location":"Pillars/Fairness/#links","text":"","title":"Links"},{"location":"Pillars/Fairness/#metrics","text":"","title":"Metrics"},{"location":"Pillars/Robustness/","text":"Robustness Summary Links Metrics","title":"Robustness"},{"location":"Pillars/Robustness/#robustness","text":"","title":"Robustness"},{"location":"Pillars/Robustness/#summary","text":"","title":"Summary"},{"location":"Pillars/Robustness/#links","text":"","title":"Links"},{"location":"Pillars/Robustness/#metrics","text":"","title":"Metrics"}]}